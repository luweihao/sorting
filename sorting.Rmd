---
title: "Sorting--Decision tree, random forest and SVM"
output: html_document
---

Reference book:  [R in Action][1]  
Related packages:  

- Decision tree: [rpart][2], [rpart.plot][3], [party][4]  
- Random forest: [randomForest][5]  
- SVM: [e1071][6]  

[1]: https://www.manning.com/books/r-in-action
[2]: https://cran.r-project.org/web/packages/rpart/
[3]: https://cran.r-project.org/web/packages/rpart.plot/
[4]: https://cran.r-project.org/web/packages/party/
[5]: https://cran.r-project.org/web/packages/randomForest/
[6]: https://cran.r-project.org/web/packages/e1071/

This markdown is done with the [breast cancer databases](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)  
The following variables are contained in the dataset:  

```{r echo=FALSE, cache=TRUE}
library(knitr)
names=read.csv("names.csv")
kable(names)
```

### Addressing the Missing Values with [mi](https://cran.r-project.org/web/packages/mi/) package  

```{r message=FALSE, warning=FALSE, cache=TRUE}
### Data loading ###
breast=read.csv("breast cancer.csv")
colnames(breast)=make.names(names$Attribute)

### Addressing the missing values
library(mi)
mdf <- missing_data.frame(breast)
mdf=change(mdf, y="Bare.Nuclei", what="type", to="ordered-categorical")
df0=mi(mdf)
df=complete(df0, 1)

### Data subseting ###
df=df[,-c(1, 12)]
df$Bare.Nuclei=as.numeric(df$Bare.Nuclei)
df$Class=factor(df$Class, levels=c(2,4), labels=c("benign", "malignant"))

### 70% set for train data ###
set.seed(086)
train=sample(nrow(df), 0.7*nrow(df))
df.train=df[train, ]
df.validate=df[-train, ]
table(df.train$Class)
table(df.validate$Class)
```

### Logistic Regression

```{r cache=TRUE}
fit.logit=glm(Class~., data=df.train, family=binomial())
summary(fit.logit)  # model check
prob=predict(fit.logit, df.validate, type = "response")
logit.pred=factor(prob>.5, levels = c(FALSE, TRUE),
                  labels=c("benign", "malignant"))  # sort the validate sample
table(df.validate$Class, logit.pred, dnn=c("Actual", "Predicted"))
```

### Decision tree

```{r cache=TRUE}
library(rpart)
set.seed(086)
dtree=rpart(Class~., data = df.train, method = "class",
            parms = list(split="information")) # Spanning Tree
dtree$cptable
plotcp(dtree) # Judge the number of pruning
dtree.pruned=prune(dtree, cp=.01) # Pruning
library(rpart.plot)
prp(dtree.pruned, type = 2, extra = 104, fallen.leaves = TRUE,
    main= "Decision Tree")
dtree.pred=predict(dtree.pruned, df.validate, type="class")
table(df.validate$Class, dtree.pred, dnn=c("Actual", "Predicted"))
```

### Random forest  
For more detail, see [Leo Breiman and Adele Cutler](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)

```{r message=FALSE, warning=FALSE, cache=TRUE}
library(randomForest)
```
```{r cache=TRUE}
set.seed(086)
fit.forest=randomForest(Class~., data=df.train, importance=TRUE,
                        na.action = na.roughfix) # Generate forest
importance(fit.forest, type=2) # Given the importance of variables
forest.pred=predict(fit.forest, df.validate)
table(df.validate$Class, forest.pred, dnn=c("Actual", "Predicted"))
```